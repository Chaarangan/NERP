torch:
    device: "cpu"
    seed: 42
data:
    train_data: 'data/train.csv'
    valid_data: 'data/valid.csv'
    train_valid_split: 0.2
    test_data: ['data/test.csv']
    parameters:
        sep: ','
        quoting: 3
        shuffle: False
    limit: 0
    tags: ['B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
model: 
    archi: "bilstm-crf"
    max_len: 128 
    dropout: 0.1
    num_workers: 
    hyperparameters:
        epochs: 1
        warmup_steps: 500
        train_batch_size: 64
        valid_batch_size: 8
        lr: 0.0001
    tokenizer_parameters: 
        do_lower_case: False
    pretrained_models: 
        - roberta-base
training:
    continue_from_checkpoint: False
    checkpoint_path: "models/model.bin"
    checkpoint_tokenizer_path: "models/tokenizer"
    output_dir: "output/"
    o_tag_cr: True
    return_accuracy: False
kfold: 
    is_kfold: False
    splits: 2
    test_on_original: False
inference:
    pretrained: "roberta-base"
    model_path: "output/model.bin"
    tokenizer_path: "output/tokenizer"
    in_file_path: "data/test.csv"
    out_file_path: "data/output.csv"